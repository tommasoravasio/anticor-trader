{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704d7e34",
   "metadata": {},
   "source": [
    "# 00 â€” CRSP Data Download (S&P 500 Top 25)\n",
    "\n",
    "This notebook connects to WRDS, selects the top 25 S&P 500 stocks by market capitalization at a target date, and downloads daily returns over a window covering the last 1,276 trading days. Data and metadata are saved to `data/` for reuse in other notebooks without re-querying WRDS.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You need a `.env` file in the project root with your WRDS credentials:\n",
    "```\n",
    "WRDS_USERNAME=your_wrds_username\n",
    "WRDS_PASSWORD=your_wrds_password\n",
    "```\n",
    "\n",
    "Copy `.env.example` to `.env` and fill in your credentials. The `.env` file is gitignored for security.\n",
    "\n",
    "## Saved outputs\n",
    "\n",
    "- `data/daily_data_top25.csv` â€” Daily data (permno, date, ret, dlret, comnam, ticker)\n",
    "- `data/permno_to_name.json` â€” Mapping PERMNO â†’ Company name\n",
    "- `data/permno_to_ticker.json` â€” Mapping PERMNO â†’ Ticker\n",
    "- `data/final_dates.json` â€” List of dates (ISO) in the final window\n",
    "- `data/download_metadata.json` â€” Parameters used and result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71299d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Connecting to WRDS and setting parametersâ€¦\n",
      "Loading library list...\n",
      "Loading library list...\n",
      "Done\n",
      "âœ… Connected to WRDS.\n",
      "ğŸ“… Last trading day selected: 2024-12-31\n",
      "\n",
      "ğŸš€ Selecting universe (Top 25 S&P 500)â€¦\n",
      "âœ… Universe identified: 25 stocks.\n",
      "\n",
      "ğŸš€ Downloading daily data (RET, DLRET, COMNAM, TICKER)â€¦\n",
      "Done\n",
      "âœ… Connected to WRDS.\n",
      "ğŸ“… Last trading day selected: 2024-12-31\n",
      "\n",
      "ğŸš€ Selecting universe (Top 25 S&P 500)â€¦\n",
      "âœ… Universe identified: 25 stocks.\n",
      "\n",
      "ğŸš€ Downloading daily data (RET, DLRET, COMNAM, TICKER)â€¦\n",
      "âœ… Download completed.\n",
      "\n",
      "ğŸ’¾ Saving outputs to ../data/ â€¦\n",
      "ğŸ“ Files written:\n",
      " - ../data/daily_data_top25.csv\n",
      " - ../data/permno_to_name.json\n",
      " - ../data/permno_to_ticker.json\n",
      " - ../data/final_dates.json\n",
      " - ../data/download_metadata.json\n",
      "ğŸ”’ WRDS connection closed.\n",
      "\n",
      "ğŸ‰ Done. You can now load these files from ../data/ in your other notebooks.\n",
      "âœ… Download completed.\n",
      "\n",
      "ğŸ’¾ Saving outputs to ../data/ â€¦\n",
      "ğŸ“ Files written:\n",
      " - ../data/daily_data_top25.csv\n",
      " - ../data/permno_to_name.json\n",
      " - ../data/permno_to_ticker.json\n",
      " - ../data/final_dates.json\n",
      " - ../data/download_metadata.json\n",
      "ğŸ”’ WRDS connection closed.\n",
      "\n",
      "ğŸ‰ Done. You can now load these files from ../data/ in your other notebooks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (look in parent directory)\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "end_date = '2025-01-30'              # Desired calendar date (will be adjusted to last trading day)\n",
    "trading_days = 1276                  # ~ 5 years of trading days\n",
    "num_stocks = 25                      # Universe size\n",
    "start_date_approx = (pd.to_datetime(end_date) - pd.to_timedelta(trading_days * 1.8, unit='d')).strftime('%Y-%m-%d')\n",
    "\n",
    "print('ğŸš€ Connecting to WRDS and setting parametersâ€¦')\n",
    "\n",
    "# Get WRDS credentials from environment variables\n",
    "wrds_username = os.getenv('WRDS_USERNAME')\n",
    "wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "\n",
    "if not wrds_username:\n",
    "    raise ValueError(\"WRDS_USERNAME not found in .env file. Please add it.\")\n",
    "\n",
    "# Connect to WRDS with credentials\n",
    "if wrds_password:\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "else:\n",
    "    # If no password is provided, WRDS will prompt or use pgpass\n",
    "    db = wrds.Connection(wrds_username=wrds_username)\n",
    "\n",
    "print('âœ… Connected to WRDS.')\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: FIND LAST TRADING DAY <= end_date\n",
    "# =============================================================================\n",
    "query_last_date = f\"SELECT max(date) as last_date FROM crsp.dsf WHERE date <= '{end_date}'\"\n",
    "last_date_df = db.raw_sql(query_last_date, date_cols=['last_date'])\n",
    "last_trading_day_str = last_date_df['last_date'][0].strftime('%Y-%m-%d')\n",
    "end_date = last_trading_day_str\n",
    "print(f'ğŸ“… Last trading day selected: {end_date}')\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: BUILD UNIVERSE (TOP 25 S&P 500 by market cap)\n",
    "# =============================================================================\n",
    "print('\\nğŸš€ Selecting universe (Top 25 S&P 500)â€¦')\n",
    "query_universe = f\"\"\"\n",
    "    WITH sp500_constituents AS (\n",
    "        SELECT permno FROM crsp.msp500list WHERE '{end_date}' BETWEEN start AND ending\n",
    "    ),\n",
    "    market_cap AS (\n",
    "        SELECT a.permno, ABS(a.prc * a.shrout) as mktcap\n",
    "        FROM crsp.dsf AS a\n",
    "        JOIN sp500_constituents AS b ON a.permno = b.permno\n",
    "        WHERE a.date = '{end_date}' AND a.prc IS NOT NULL AND a.shrout IS NOT NULL\n",
    "    )\n",
    "    SELECT permno FROM market_cap ORDER BY mktcap DESC LIMIT {num_stocks}\n",
    "\"\"\"\n",
    "top_25_permno = db.raw_sql(query_universe)['permno'].tolist()\n",
    "permno_tuple = tuple(top_25_permno)\n",
    "print(f'âœ… Universe identified: {len(top_25_permno)} stocks.')\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: DOWNLOAD HISTORICAL DATA (RET, DLRET, + LABELS)\n",
    "# =============================================================================\n",
    "print('\\nğŸš€ Downloading daily data (RET, DLRET, COMNAM, TICKER)â€¦')\n",
    "query_data = f\"\"\"\n",
    "    SELECT a.permno, a.date, a.ret, b.dlret, c.comnam, c.ticker\n",
    "    FROM crsp.dsf AS a\n",
    "    LEFT JOIN crsp.dsedelist AS b\n",
    "        ON a.permno = b.permno AND a.date = b.dlstdt\n",
    "    LEFT JOIN crsp.msenames AS c\n",
    "        ON a.permno = c.permno AND a.date BETWEEN c.namedt AND c.nameendt\n",
    "    WHERE a.permno IN {permno_tuple} AND a.date BETWEEN '{start_date_approx}' AND '{end_date}'\n",
    "\"\"\"\n",
    "daily_data = db.raw_sql(query_data, date_cols=['date'])\n",
    "\n",
    "# Fill missing comnam/ticker labels by PERMNO (ffill/bfill to cover gaps)\n",
    "daily_data['comnam'] = daily_data.groupby('permno')['comnam'].transform(lambda x: x.ffill().bfill())\n",
    "daily_data['ticker'] = daily_data.groupby('permno')['ticker'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "# Create useful mappings (convert numpy int64 keys to regular Python int for JSON compatibility)\n",
    "permno_to_name = {int(k): v for k, v in daily_data.drop_duplicates('permno').set_index('permno')['comnam'].to_dict().items()}\n",
    "permno_to_ticker = {int(k): v for k, v in daily_data.drop_duplicates('permno').set_index('permno')['ticker'].to_dict().items()}\n",
    "\n",
    "# Limit to the last 1,276 trading days actually present\n",
    "all_dates = sorted(daily_data['date'].unique())\n",
    "final_dates = all_dates[-trading_days:] if len(all_dates) >= trading_days else all_dates\n",
    "daily_data = daily_data[daily_data['date'].isin(final_dates)]\n",
    "\n",
    "print('âœ… Download completed.')\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO ../data/ (parent directory)\n",
    "# =============================================================================\n",
    "print('\\nğŸ’¾ Saving outputs to ../data/ â€¦')\n",
    "data_dir = os.path.join('..', 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "daily_csv_path = os.path.join(data_dir, 'daily_data_top25.csv')\n",
    "permno_to_name_path = os.path.join(data_dir, 'permno_to_name.json')\n",
    "permno_to_ticker_path = os.path.join(data_dir, 'permno_to_ticker.json')\n",
    "final_dates_path = os.path.join(data_dir, 'final_dates.json')\n",
    "meta_path = os.path.join(data_dir, 'download_metadata.json')\n",
    "\n",
    "# Convert dates to ISO for JSON\n",
    "final_dates_iso = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in final_dates]\n",
    "\n",
    "# Save files\n",
    "daily_data.to_csv(daily_csv_path, index=False)\n",
    "with open(permno_to_name_path, 'w', encoding='utf-8') as f: \n",
    "    json.dump(permno_to_name, f, ensure_ascii=False, indent=2)\n",
    "with open(permno_to_ticker_path, 'w', encoding='utf-8') as f: \n",
    "    json.dump(permno_to_ticker, f, ensure_ascii=False, indent=2)\n",
    "with open(final_dates_path, 'w', encoding='utf-8') as f: \n",
    "    json.dump(final_dates_iso, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "metadata = {\n",
    "    'generated_at': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "    'parameters': {\n",
    "        'requested_end_date': end_date,\n",
    "        'trading_days_target': trading_days,\n",
    "        'num_stocks': num_stocks,\n",
    "        'start_date_approx': start_date_approx\n",
    "    },\n",
    "    'universe': {\n",
    "        'count': len(top_25_permno),\n",
    "        'permnos': top_25_permno\n",
    "    },\n",
    "    'date_window': {\n",
    "        'start': final_dates_iso[0] if final_dates_iso else None,\n",
    "        'end': final_dates_iso[-1] if final_dates_iso else None,\n",
    "        'n_trading_days': len(final_dates_iso)\n",
    "    },\n",
    "    'files': {\n",
    "        'daily_csv': daily_csv_path,\n",
    "        'permno_to_name': permno_to_name_path,\n",
    "        'permno_to_ticker': permno_to_ticker_path,\n",
    "        'final_dates': final_dates_path\n",
    "    }\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f: \n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('ğŸ“ Files written:')\n",
    "print(' -', daily_csv_path)\n",
    "print(' -', permno_to_name_path)\n",
    "print(' -', permno_to_ticker_path)\n",
    "print(' -', final_dates_path)\n",
    "print(' -', meta_path)\n",
    "\n",
    "# =============================================================================\n",
    "# CLOSE WRDS CONNECTION\n",
    "# =============================================================================\n",
    "db.close()\n",
    "print('ğŸ”’ WRDS connection closed.')\n",
    "\n",
    "print('\\nğŸ‰ Done. You can now load these files from ../data/ in your other notebooks.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
